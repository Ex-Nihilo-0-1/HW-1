# HW 1

#1. Did you alter the Node data structure? If so, how and why?

We did alter the Node data structure. We added `self.decision_label` in the node's init function and added two helper function: `add_label` and `add_decision_label`.

We have these two function so that we can use the add_label function for all the leaf node in the decision tree, and `add_decision_label` for nodes that are not terminal and are used to determine which attribute will be used for that node.

#2. How did you handle missing attributes, and why did you choose this strategy?

We handle missing attribute by either skipping it upon detection or create substitute attributes using the most common class label to fill them. 

We use this strategy because it's a simple solution that provided the best stability.

#3. How did you perform pruning, and why did you choose this strategy?

We choose the strategy reduced error pruning because it has a simple heuristic and has reliable generalization performance. Its good for situation with limited data and handles small data sets well.

# 5.

Here's the result

```
training accuracy:  1.0
validation accuracy:  0.7714285714285715
test accuracy:  0.8571428571428571
pruned tree train accuracy:  0.805
pruned tree validation accuracy:  0.7714285714285715
pruned tree test accuracy:  0.7714285714285715
no pruning test accuracy:  0.8571428571428571
```

As you can see, the training accuracy reached 1.0.

The 100% training accuracy indicates overfitting, where the model learns both the patterns and noise of the training data, leading to lower performance on unseen data.

After prunning, we encountered slightly lower accuracy compared to the unpruned model on the test set. Theoretically prunned data should provided better balance between generalization and performance, but we are also baffled as to the reason why pruned data cause consistent lower accuracy on testing data

#6. 
Design of random forest:
We chose to train 10 trees in the forest because the dataset for candy is not large, so ten trees is a suitable number of trees to balance the cost and performance.
Each tree is trained on a sample of the original dataset, which creates different subsets for training. We are using the random feature to increase the accuracy. Random features will help reduce the correlation which can reduce overfitting. Each tree randomly selects the features to split the node, which will make the forest more robust. For the prediction, we used the ID3 algorithm to train each tree and did the comparison based on information gain, which can lead to more effective classification.

Accuracy comparison:

ID3 Decision Tree Accuracy: 47.06%
Random Forest Accuracy: 82.35%

The comparison between the results of a single decision tree generated by ID3 and the random forest is achieved by using the evaluation function from randomForest class and the test function from the ID3 class. Using the predictAll method, a list of predictions is generated using the test set. Compared with the actual value from the test set, accuracy is gained for the randomForest prediction. Compared with the accuracy of the single decision tree prediction from the ID3, we conclude that the randomForest prediction is more accurate than the ID3 prediction.


